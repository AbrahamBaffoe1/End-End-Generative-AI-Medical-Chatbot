{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pypdf\n",
    "!pip install -U pypdf2\n",
    "!pip install -U langchain\n",
    "!pip install -U langchain-experimental\n",
    "!pip install -U langchain-community\n",
    "!pip install -U langchain-pinecone\n",
    "!pip install -U sentence-transformers\n",
    "!pip install -U pinecone-client\n",
    "!pip install -U openai\n",
    "!pip install -U pandas\n",
    "!pip install -U numpy\n",
    "!pip install -U scikit-learn\n",
    "!pip install -U matplotlib\n",
    "!pip install -U seaborn\n",
    "!pip install -U gunicorn\n",
    "!pip install -U python-dotenv\n",
    "!pip install -U flask-cors\n",
    "!pip install -U requests\n",
    "!pip install -U openai\n",
    "!pip install -U python-dotenv\n",
    "!pip install -U flask-cors\n",
    "!pip install -U requests\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let extract the data from the pdf file using fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                            glob='*.pdf',\n",
    "                            loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the content from the pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "extracted_data = load_data(data='/Users/kwamebaffoe/Desktop/End-End-Generative-AI-Medical-Chatbot/Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data\n",
    "\n",
    "# Let Split the data into chunks using fucntions\n",
    "def split_data(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = split_data(extracted_data)\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let download the embeddings from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings  \n",
    "\n",
    "def download_huggingface_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers\n",
    "embeddings = download_huggingface_embeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Let's convert to Vector Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings  # Ensure you import this\n",
    "\n",
    "def download_huggingface_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "# Call the function to get the embeddings\n",
    "embeddings = download_huggingface_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CODES\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import cached_download\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can use the embeddings to embed the query\n",
    "# !pip install sentence-transformers\n",
    "import sentence_transformers\n",
    "\n",
    "query_output = embeddings.embed_query(\"What is a heart attack?\")\n",
    "print(\"Length of query output: \", len(query_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_output\n",
    "\n",
    "# !pip install google\n",
    "# !pip install \"sentence-transformers>=3.0.0\"\n",
    "# !pip install sentence-transformers\n",
    "# !pip install \"torch>=1.11.0\"\n",
    "\n",
    "# !pip install torch==1.11.0\n",
    "# !pip install sentence-transformers\n",
    "\n",
    "# Example: Install torch with a specified CUDA or CPU version\n",
    "# !pip install torch==1.11.0+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\n",
    "\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from huggingface_hub import cached_download\n",
    "\n",
    "# !pip install sentence-transformers==2.2.2 huggingface-hub==0.12.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Index with Pinecone Using Code Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import SeverlessSpec\n",
    "import os\n",
    "\n",
    "\n",
    "#  Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"PINECONE_API_KEY\"),\n",
    "        \n",
    "index = \"einseinnmedicalchatbot\"\n",
    "\n",
    "pc.create_index(name=\"index_name\",\n",
    "                dimension=384,\n",
    "                metric=\"cosine\",\n",
    "                spec=SeverlessSpec(),\n",
    "                cloud=\"aws\", \n",
    "                region=\"us-east-1\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let create the Index with Python Codes in Simple Way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_2jGdBi_RFTHxF67eEHmau3ecngztXbgM8EGVQ1et1SMAJtom2t99yvUUZTWtRYGYAT4Gpd\")\n",
    "\n",
    "index_name = \"einseinnmedicalchatbot\"\n",
    "\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, \n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")\n",
    "\n",
    "# !pip install --upgrade pinecone-client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVERT THE CHUNKS INTO VECTORS EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "docs_searching = Pinecone.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=\"einseinnmedicalchatbot\"  # Note: this should be a string\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let load an exisiting Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "docs_searching = Pinecone.from_existing_index(\n",
    "    embedding=embeddings,\n",
    "    index_name=\"einseinnmedicalchatbot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_searching.similarity_search(\"What is a heart attack?\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = docs_searching.similarity_search(\"What is a heart attack?\", k=2)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docs_searching.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_docs = retriever.invoke( \"What is an Acne?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let make Sure the Query Goes to the Knowledge Base Model before it goes to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using OpenAI LLM model for this\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.4, max_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid template: input_variables=['context', 'input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are a helpful assistant for question and answering task about medicine. Use the following pieces of retrieved context to answer the question. Say No to questions that are not about medicine. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum and keep the answer as short as possible.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 17\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[1;32m      5\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[1;32m      6\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      7\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant for question and answering task about medicine. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m ])\n\u001b[0;32m---> 17\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{input}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/einstienMed/lib/python3.10/site-packages/langchain_core/prompts/chat.py:1208\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_messages\u001b[0;34m(cls, messages, template_format)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_messages\u001b[39m(\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   1170\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m   1171\u001b[0m     template_format: PromptTemplateFormat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1172\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;124;03m        a chat prompt template.\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/einstienMed/lib/python3.10/site-packages/langchain_core/prompts/chat.py:1004\u001b[0m, in \u001b[0;36mChatPromptTemplate.__init__\u001b[0;34m(self, messages, template_format, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    952\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    956\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \n\u001b[1;32m    959\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \n\u001b[1;32m   1003\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1005\u001b[0m         _convert_to_message(message, template_format) \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m   1006\u001b[0m     ]\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     input_vars: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/einstienMed/lib/python3.10/site-packages/langchain_core/prompts/chat.py:1005\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    952\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    956\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \n\u001b[1;32m    959\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \n\u001b[1;32m   1003\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m-> 1005\u001b[0m         \u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m   1006\u001b[0m     ]\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     input_vars: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/einstienMed/lib/python3.10/site-packages/langchain_core/prompts/chat.py:1476\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[0;34m(message, template_format)\u001b[0m\n\u001b[1;32m   1474\u001b[0m message_type_str, template \u001b[38;5;241m=\u001b[39m message\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message_type_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1476\u001b[0m     _message \u001b[38;5;241m=\u001b[39m \u001b[43m_create_template_from_message_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_type_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1480\u001b[0m     _message \u001b[38;5;241m=\u001b[39m message_type_str(\n\u001b[1;32m   1481\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m   1482\u001b[0m             cast(\u001b[38;5;28mstr\u001b[39m, template), template_format\u001b[38;5;241m=\u001b[39mtemplate_format\n\u001b[1;32m   1483\u001b[0m         )\n\u001b[1;32m   1484\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/einstienMed/lib/python3.10/site-packages/langchain_core/prompts/chat.py:1389\u001b[0m, in \u001b[0;36m_create_template_from_message_type\u001b[0;34m(message_type, template, template_format)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     message \u001b[38;5;241m=\u001b[39m AIMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m   1386\u001b[0m         cast(\u001b[38;5;28mstr\u001b[39m, template), template_format\u001b[38;5;241m=\u001b[39mtemplate_format\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m message_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1389\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mSystemMessagePromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m message_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaceholder\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(template, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/einstienMed/lib/python3.10/site-packages/langchain_core/prompts/chat.py:580\u001b[0m, in \u001b[0;36m_StringImageMessagePromptTemplate.from_template\u001b[0;34m(cls, template, template_format, partial_variables, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    579\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid template: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid template: input_variables=['context', 'input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are a helpful assistant for question and answering task about medicine. Use the following pieces of retrieved context to answer the question. Say No to questions that are not about medicine. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum and keep the answer as short as possible.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are a helpful assistant for question and answering task about medicine. \"\n",
    "     \"Use the following pieces of retrieved context to answer the question. \"\n",
    "     \"Say No to questions that are not about medicine. \"\n",
    "     \"If you don't know the answer, just say that you don't know, don't try to make up an answer. \"\n",
    "     \"Use three sentences maximum and keep the answer as short as possible.\\n\\n\"\n",
    "     \"{context}\"\n",
    "    ),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the document chain\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template\n",
    ")\n",
    "\n",
    "# Create the retrieval chain\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain=document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let Create a ChatBot Using LangChain \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the OpenAI API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Now create your ChatOpenAI instance\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9)\n",
    "chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)\n",
    "\n",
    "# Test the chain\n",
    "response = chain(\n",
    "    {\"question\": \"What is a heart attack?\"}, \n",
    "    {\"chat_history\": []}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "einstienMed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
